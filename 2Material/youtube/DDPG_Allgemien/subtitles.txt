WEBVTT
Kind: captions
Language: en

 
all right before moving on from dqn

all right before moving on from dqn
 

all right before moving on from dqn
completely

completely
 

completely
let's uh think about what dqn would look

let's uh think about what dqn would look
 

let's uh think about what dqn would look
like

like
 

like
with environments that require

with environments that require
 

with environments that require
continuous

continuous
 

continuous
action spaces and then see whether we

action spaces and then see whether we
 

action spaces and then see whether we
can do something to actually alleviate

can do something to actually alleviate
 

can do something to actually alleviate
the problems that arise in that context

the problems that arise in that context
 

the problems that arise in that context
all right so this is how our deep q

all right so this is how our deep q
 

all right so this is how our deep q
learning network looks like so far

learning network looks like so far
 

learning network looks like so far
we have an input state s and its outputs

we have an input state s and its outputs
 

we have an input state s and its outputs
are going to be the q values

are going to be the q values
 

are going to be the q values
corresponding to different actions

corresponding to different actions
 

corresponding to different actions
within our

within our
 

within our
action space and because we've been

action space and because we've been
 

action space and because we've been
dealing primarily with discrete actions

dealing primarily with discrete actions
 

dealing primarily with discrete actions
so far we've not explicitly mentioned

so far we've not explicitly mentioned
 

so far we've not explicitly mentioned
this but really we've been dealing with

this but really we've been dealing with
 

this but really we've been dealing with
discrete actions so far because that's

discrete actions so far because that's
 

discrete actions so far because that's
the only way

the only way
 

the only way
you can enumerate all the actions uh in

you can enumerate all the actions uh in
 

you can enumerate all the actions uh in
this manner

this manner
 

this manner
right so from a equals one to a equals

right so from a equals one to a equals
 

right so from a equals one to a equals
capital a where you have some finite

capital a where you have some finite
 

capital a where you have some finite
size capital a of the the space of

size capital a of the the space of
 

size capital a of the the space of
actions

actions
 

actions
um and therefore we are able to have

um and therefore we are able to have
 

um and therefore we are able to have
exactly that many output output units

exactly that many output output units
 

exactly that many output output units
each of those output units produces the

each of those output units produces the
 

each of those output units produces the
scalar corresponding to

scalar corresponding to
 

scalar corresponding to
q of s comma a equals 1 a equals 2 and

q of s comma a equals 1 a equals 2 and
 

q of s comma a equals 1 a equals 2 and
so on

so on
 

so on
right so this is how we've parameterized

right so this is how we've parameterized
 

right so this is how we've parameterized
deep q networks

deep q networks
 

deep q networks
so far with these discrete action spaces

so far with these discrete action spaces
 

so far with these discrete action spaces
and before looking at what happens in

and before looking at what happens in
 

and before looking at what happens in
the continuous action setting let's look

the continuous action setting let's look
 

the continuous action setting let's look
at

at
 

at
how we actually use this deep queue

how we actually use this deep queue
 

how we actually use this deep queue
network to select

network to select
 

network to select
an action eventually in particular

an action eventually in particular
 

an action eventually in particular
remember we said that to actually use

remember we said that to actually use
 

remember we said that to actually use
the dq

the dq
 

the dq
dpq network values to select actions

dpq network values to select actions
 

dpq network values to select actions
to convert it into a policy and and

to convert it into a policy and and
 

to convert it into a policy and and
select the optimal actions in the

select the optimal actions in the
 

select the optimal actions in the
environment

environment
 

environment
all that you'd have to do is take the r

all that you'd have to do is take the r
 

all that you'd have to do is take the r
max over a of q of s comma a

max over a of q of s comma a
 

max over a of q of s comma a
and in this setting it's very simple

and in this setting it's very simple
 

and in this setting it's very simple
because all that you have to do is

because all that you have to do is
 

because all that you have to do is
given that all the outputs are produced

given that all the outputs are produced
 

given that all the outputs are produced
in one shot one pass through the network

in one shot one pass through the network
 

in one shot one pass through the network
one forward pass through the network

one forward pass through the network
 

one forward pass through the network
then you just take the maximum of all of

then you just take the maximum of all of
 

then you just take the maximum of all of
those discrete

those discrete
 

those discrete
this finite set of values and you will

this finite set of values and you will
 

this finite set of values and you will
get your a star

get your a star
 

get your a star
and that's also your policy piops

and that's also your policy piops
 

and that's also your policy piops
now this policy

now this policy
 

now this policy
is is used for selecting actions after

is is used for selecting actions after
 

is is used for selecting actions after
having trained the queue network but

having trained the queue network but
 

having trained the queue network but
remember that we're doing actually

remember that we're doing actually
 

remember that we're doing actually
something quite similar even during

something quite similar even during
 

something quite similar even during
training time

training time
 

training time
um in particular at training time we set

um in particular at training time we set
 

um in particular at training time we set
the q targets

the q targets
 

the q targets
as the bellman targets and the bellman

as the bellman targets and the bellman
 

as the bellman targets and the bellman
targets are the current reward plus the

targets are the current reward plus the
 

targets are the current reward plus the
discount factor times

discount factor times
 

discount factor times
the maximum over the q5 for the next

the maximum over the q5 for the next
 

the maximum over the q5 for the next
state and action right and

state and action right and
 

state and action right and
this looks an awful lot like what we

this looks an awful lot like what we
 

this looks an awful lot like what we
were doing over here in particular you

were doing over here in particular you
 

were doing over here in particular you
can explicitly write it

can explicitly write it
 

can explicitly write it
as the q phi of s i prime

as the q phi of s i prime
 

as the q phi of s i prime
and pi over s i prime pi being the r max

and pi over s i prime pi being the r max
 

and pi over s i prime pi being the r max
right so convince yourself that this

right so convince yourself that this
 

right so convince yourself that this
maximization

maximization
 

maximization
can be taken inside as an arg max and

can be taken inside as an arg max and
 

can be taken inside as an arg max and
when you do that

when you do that
 

when you do that
that this is exactly the same as setting

that this is exactly the same as setting
 

that this is exactly the same as setting
a i prime over here to pi of s prime

 
 

 
okay so this is obviously an operation

okay so this is obviously an operation
 

okay so this is obviously an operation
that we are repeating several times

that we are repeating several times
 

that we are repeating several times
because we're using it at training time

because we're using it at training time
 

because we're using it at training time
after all so we're repeating it very

after all so we're repeating it very
 

after all so we're repeating it very
very often

very often
 

very often
um and it better be the case that we're

um and it better be the case that we're
 

um and it better be the case that we're
able to compute this

able to compute this
 

able to compute this
very quickly and in the discrete case

very quickly and in the discrete case
 

very quickly and in the discrete case
this is very much the case we're able to

this is very much the case we're able to
 

this is very much the case we're able to
compute it quite quickly

compute it quite quickly
 

compute it quite quickly
like we just went through all that you

like we just went through all that you
 

like we just went through all that you
have to do is take the maximum of a

have to do is take the maximum of a
 

have to do is take the maximum of a
discrete set of values

 
 

 
so what happens if we try and port the

so what happens if we try and port the
 

so what happens if we try and port the
same solution

same solution
 

same solution
into the continuous action setting well

into the continuous action setting well
 

into the continuous action setting well
in the continuous action setting there

in the continuous action setting there
 

in the continuous action setting there
is no meaningful way to enumerate the

is no meaningful way to enumerate the
 

is no meaningful way to enumerate the
actions

actions
 

actions
you can't just list down a finite set of

you can't just list down a finite set of
 

you can't just list down a finite set of
actions

actions
 

actions
and so you can't really do this anymore

and so you can't really do this anymore
 

and so you can't really do this anymore
and so

and so
 

and so
you would have to instead come up with a

you would have to instead come up with a
 

you would have to instead come up with a
q network that looks a little bit

q network that looks a little bit
 

q network that looks a little bit
different

different
 

different
in particular you could take the state s

in particular you could take the state s
 

in particular you could take the state s
and the action a as input the continuous

and the action a as input the continuous
 

and the action a as input the continuous
action a as input

action a as input
 

action a as input
and you would output q of s comma i

and you would output q of s comma i
 

and you would output q of s comma i
that seems like it solved this problem

that seems like it solved this problem
 

that seems like it solved this problem
of not having not having the ability to

of not having not having the ability to
 

of not having not having the ability to
enumerate q of s comma a

enumerate q of s comma a
 

enumerate q of s comma a
but you still have a problem now when

but you still have a problem now when
 

but you still have a problem now when
you start thinking about how you would

you start thinking about how you would
 

you start thinking about how you would
set

set
 

set
the policy action which is the r max

the policy action which is the r max
 

the policy action which is the r max
over a of q of x comma a

over a of q of x comma a
 

over a of q of x comma a
you all of a sudden see that there is no

you all of a sudden see that there is no
 

you all of a sudden see that there is no
simple way

simple way
 

simple way
in which you can take this r max and it

in which you can take this r max and it
 

in which you can take this r max and it
looks a little bit more like an

looks a little bit more like an
 

looks a little bit more like an
optimization problem

optimization problem
 

optimization problem
so taking the arc max of this network

so taking the arc max of this network
 

so taking the arc max of this network
for a fixed state and trying to find the

for a fixed state and trying to find the
 

for a fixed state and trying to find the
rmax over

rmax over
 

rmax over
the action input looks like looks like a

the action input looks like looks like a
 

the action input looks like looks like a
kind of optimization problem the same

kind of optimization problem the same
 

kind of optimization problem the same
type of optimization problem perhaps as

type of optimization problem perhaps as
 

type of optimization problem perhaps as
what we were setting

what we were setting
 

what we were setting
what we were solving when we were trying

what we were solving when we were trying
 

what we were solving when we were trying
to find the optimal weights

to find the optimal weights
 

to find the optimal weights
of a neural network to minimize the loss

of a neural network to minimize the loss
 

of a neural network to minimize the loss
function let's say

function let's say
 

function let's say
so you could apply the same kinds of

so you could apply the same kinds of
 

so you could apply the same kinds of
methods we used there like you could use

methods we used there like you could use
 

methods we used there like you could use
gradient descent for example but of

gradient descent for example but of
 

gradient descent for example but of
course

course
 

course
that's not something that converges very

that's not something that converges very
 

that's not something that converges very
quickly

quickly
 

quickly
you would have to solve this iterative

you would have to solve this iterative
 

you would have to solve this iterative
gradient descent problem every time you

gradient descent problem every time you
 

gradient descent problem every time you
wanted to take the r max of q of s comma

wanted to take the r max of q of s comma
 

wanted to take the r max of q of s comma
a and that's not feasible

a and that's not feasible
 

a and that's not feasible
because remember we want to solve this

because remember we want to solve this
 

because remember we want to solve this
optimization problem really frequently

optimization problem really frequently
 

optimization problem really frequently
like we saw in the previous slide we

like we saw in the previous slide we
 

like we saw in the previous slide we
want to use it

want to use it
 

want to use it
both when computing the eventual policy

both when computing the eventual policy
 

both when computing the eventual policy
actions but also

actions but also
 

actions but also
when setting the targets for our uh for

when setting the targets for our uh for
 

when setting the targets for our uh for
our q network

 
 

 
so this would be too expensive if we if

so this would be too expensive if we if
 

so this would be too expensive if we if
we just treated us and treated it as an

we just treated us and treated it as an
 

we just treated us and treated it as an
optimization problem that we solve from

optimization problem that we solve from
 

optimization problem that we solve from
scratch each time

scratch each time
 

scratch each time
so what else could what else could we do

so what else could what else could we do
 

so what else could what else could we do
if we don't want to repeatedly solve

if we don't want to repeatedly solve
 

if we don't want to repeatedly solve
this optimization problem

this optimization problem
 

this optimization problem
is there some way to get around it well

is there some way to get around it well
 

is there some way to get around it well
here is one potential solution

here is one potential solution
 

here is one potential solution
let's train a neural network to produce

let's train a neural network to produce
 

let's train a neural network to produce
the outputs of this optimization problem

the outputs of this optimization problem
 

the outputs of this optimization problem
now remember the output of this rmax is

now remember the output of this rmax is
 

now remember the output of this rmax is
a function of the input state

a function of the input state
 

a function of the input state
s right and so you have a separate a

s right and so you have a separate a
 

s right and so you have a separate a
star corresponding to each input s you

star corresponding to each input s you
 

star corresponding to each input s you
have a separate optimal action

have a separate optimal action
 

have a separate optimal action
corresponding to each input s

corresponding to each input s
 

corresponding to each input s
so why not train a network

so why not train a network
 

so why not train a network
that maps from the input state to this

that maps from the input state to this
 

that maps from the input state to this
output action a star which is the

output action a star which is the
 

output action a star which is the
which is the solution to this

which is the solution to this
 

which is the solution to this
optimization problem

optimization problem
 

optimization problem
so what would that look like well it

so what would that look like well it
 

so what would that look like well it
would look like a network that takes in

would look like a network that takes in
 

would look like a network that takes in
s as input

s as input
 

s as input
produces a star as output now

produces a star as output now
 

produces a star as output now
remember that this mapping from the

remember that this mapping from the
 

remember that this mapping from the
state to the optimal action is exactly

state to the optimal action is exactly
 

state to the optimal action is exactly
what the optimal policy is supposed to

what the optimal policy is supposed to
 

what the optimal policy is supposed to
be doing

be doing
 

be doing
so let's just call this the policy

so let's just call this the policy
 

so let's just call this the policy
network pi

network pi
 

network pi
all right

 
 

 
now we've only so far set up that the

now we've only so far set up that the
 

now we've only so far set up that the
inputs are states

inputs are states
 

inputs are states
s and the outputs are the optimal

s and the outputs are the optimal
 

s and the outputs are the optimal
actions a star

actions a star
 

actions a star
how are we going to train this we're

how are we going to train this we're
 

how are we going to train this we're
going to train this to maximize

going to train this to maximize
 

going to train this to maximize
the q function right

the q function right
 

the q function right
and we are parameterizing the q function

and we are parameterizing the q function
 

and we are parameterizing the q function
of course because we're

of course because we're
 

of course because we're
we're we're operating with dq n we're

we're we're operating with dq n we're
 

we're we're operating with dq n we're
parameterizing the queue function in a

parameterizing the queue function in a
 

parameterizing the queue function in a
queue network

queue network
 

queue network
and so now we have all the components

and so now we have all the components
 

and so now we have all the components
necessary

necessary
 

necessary
we're going to train the queue network

we're going to train the queue network
 

we're going to train the queue network
just like we normally do to

just like we normally do to
 

just like we normally do to
to to by using the standard

to to by using the standard
 

to to by using the standard
uh squared bellman error loss right

uh squared bellman error loss right
 

uh squared bellman error loss right
and the policy network is going to be

and the policy network is going to be
 

and the policy network is going to be
trained in turn to minimize

trained in turn to minimize
 

trained in turn to minimize
q of s comma a with respect to this this

q of s comma a with respect to this this
 

q of s comma a with respect to this this
queue that we are currently learning

 
 

 
and it's very common to call this kind

and it's very common to call this kind
 

and it's very common to call this kind
of setup an

of setup an
 

of setup an
actor critic setup where the policy is

actor critic setup where the policy is
 

actor critic setup where the policy is
called the actor

called the actor
 

called the actor
because obviously it produces the

because obviously it produces the
 

because obviously it produces the
actions and

actions and
 

actions and
the q network is called the critic

the q network is called the critic
 

the q network is called the critic
because you can think of it as

because you can think of it as
 

because you can think of it as
evaluating

evaluating
 

evaluating
a state action tuple and saying how good

a state action tuple and saying how good
 

a state action tuple and saying how good
it is

it is
 

it is
which is exactly what q of s comma a is

which is exactly what q of s comma a is
 

which is exactly what q of s comma a is
remember q of s comma a

remember q of s comma a
 

remember q of s comma a
is the expected utility of producing

is the expected utility of producing
 

is the expected utility of producing
action a

action a
 

action a
from state s with the optimal policy

from state s with the optimal policy
 

from state s with the optimal policy
we're not saying this explicitly anymore

we're not saying this explicitly anymore
 

we're not saying this explicitly anymore
but these are q stars

but these are q stars
 

but these are q stars
right so this is the actor this is the

right so this is the actor this is the
 

right so this is the actor this is the
critic

critic
 

critic
this is our very first example of an

this is our very first example of an
 

this is our very first example of an
actor critic algorithm

actor critic algorithm
 

actor critic algorithm
and this algorithm where you take dqn

and this algorithm where you take dqn
 

and this algorithm where you take dqn
and modify it in this way to work well

and modify it in this way to work well
 

and modify it in this way to work well
with continuous actions

with continuous actions
 

with continuous actions
is called deep deterministic policy

is called deep deterministic policy
 

is called deep deterministic policy
gradients or ddpg and we saw an example

gradients or ddpg and we saw an example
 

gradients or ddpg and we saw an example
of how this works earlier

of how this works earlier
 

of how this works earlier
with the robotics setups

with the robotics setups
 

with the robotics setups
in robotics typically you will very

in robotics typically you will very
 

in robotics typically you will very
often need to set some continuous

often need to set some continuous
 

often need to set some continuous
actions like for example

actions like for example
 

actions like for example
the torques in various motors of your

the torques in various motors of your
 

the torques in various motors of your
robot arm for example

robot arm for example
 

robot arm for example
and you'll remember we saw an example

and you'll remember we saw an example
 

and you'll remember we saw an example
earlier of how that works

earlier of how that works
 

earlier of how that works
and that was actually in fact employing

and that was actually in fact employing
 

and that was actually in fact employing
ddpg

