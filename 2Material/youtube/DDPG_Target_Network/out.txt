WEBVTT
Kind: captions
Language: en

 
this video is to explain the DPG in
reinforcement learning DD PG means the
deep deterministic policy gradient it's
most important from deterministic is in
contrast with stochastic which means
that we have a policy network that under
a certain environment observation it
only gives the one that believes to be
the most appropriate action for example
if we are to apply the DBZ to a
self-driving car the observation can
consist of the current because of
velocity and steering velocity the
relative position to the next target
which can be X Y and theta and the
obstacles that can be encoded into a set
of angle and distance the action can be
a vector of forward and lateral
acceleration let's have a look at the
basic elements of the DD BG it uses the
structure of actor Craddock which means
it has two networks for the actor
network it takes the observation and
gives the action for the critique
corresponding action and outputs the Q
value the Q value measurements how good
the action is in detail the Q value
equals to the reward for the current
action plus a discounted Q next the Q
next means the optimal fuel for the next
observation and action and for the
discount value in most cases in equal to
0 99 also so DT PG is off policy and it
has a replay before such that it can be
trained only using real-world experience
the replay buffer recalls a series of
the observation action reward and the
next observation
now let's see how to train the actor
network that's pretty easy assume that
we already have a well trained
accredited work and what we are going to
do is to move the actor Network just
another critic Network and then fade
into the real world observations we can
have that Q value what we are going to
do next is to maximize the Q value
output or to say minimize the minus Q in
this way we can have the actor network
trained as well next is how to train the
critic we already know that Q equals the
reward plus a discounted Q next what we
can see is that we already have the Q
value here we also have the reward
what we want is the Q next so what we
are going to do is to reuse this critic
network how I just made a copy and move
it here and we also going to use this
actor network let's move it just another
critic network so what we are going to
do is to fit in the next observation and
just mark it next next and the output
from this actor is the optimal action
for the next observation let's market to
so what we get now is the next cue as we
can see here is that we already have the
cue next the reward and the cue how are
we going to try the credit is that we
are going to make the both side of the
equation getting closer which means to
minimize the difference between Q and
reward
Plus discounted next Q a problem here is
that if these two critics have comes
from the same network since update of
this network makes a q and q next change
in the same time which makes the
training process more unlikely to
converge such that DDP G is also using a
target network to solve this problem to
implement target network the delivery
has two sets of actor and critics when
training the actor it freezes the critic
and fit in the observation making the
queue getting larger
so the Hector is getting updated when
training the credits it fails in the
observation and action also fears the
next observation to the target network
getting the cue next and trying to make
the q equals to the reward Plaza
discounted q next and so the critic
network is also getting updated
and finally the trend module parameters
will be slowly updated to the target
network using a moving average Dale
finally it's expiration it's obvious
that the model is only capable of making
totally random actions in the beginning
so if you are using RL to play game
general is a player well just kill
yourself instantly when the game starts
well the actor is being creamed the
player is more likely to make reasonable
actions which will unfold most divisions
my over action noise can be added to the
output of the actor and parameter noise
can also be directly injected into the
actor network just like the evolution
strategy
hello that's a house thanks for watching
this is my first video with a lot of
technical issues the background noise
and wahla means keep changing plasma
English is not that good
so I have to record again again however
it is hopefully can be useful and see
you in next videos bye
