\subsection{Rückpropagation in Neuronalen Netzwerken}

Zur Minimierung der Kostenfunktion \( C_0 \), die wie folgt definiert ist:

\begin{equation}
C_0 = \sum_{j=0}^{n_{L-1}} (a_j^{[L]} - y_j)^2
\end{equation}

wird die Technik der Rückpropagation verwendet. Das grundlegende Konzept besteht darin, den Fehler von der Ausgabeschicht rückwärts durch das Netzwerk zu propagieren.

\paragraph{Fehler in der Ausgabeschicht}

Der Fehler in der Ausgabeschicht wird durch folgende Formel definiert:

\[
\frac{\partial C_0}{\partial a_j^{[L]}} = 2 \left( a_j^{[L]} - y_j \right)
\]

\paragraph{Fehler Rückpropagieren}

Die Ableitungen der Aktivierungsfunktion \( a_j^{[L]} \) und der linearen Kombination \( z_j^{[L]} \) sind:

\[
\frac{\partial a_j^{[L]}}{\partial z_j^{[L]}} = \sigma' \left( z_j^{[L]} \right)
\]
\[
\frac{\partial z_j^{[L]}}{\partial w_{jk}^{[L]}} = a_k^{[L-1]}
\]

\paragraph{Kettenregel Anwenden}

Nun kombinieren wir alle diese Teile mit der Kettenregel:

\[
\frac{\partial C_0}{\partial w_{jk}^{[L]}} = \frac{\partial C_0}{\partial a_j^{[L]}} \cdot \frac{\partial a_j^{[L]}}{\partial z_j^{[L]}} \cdot \frac{\partial z_j^{[L]}}{\partial w_{jk}^{[L]}}\]

Durch Anwendung der Kettenregel erhalten wir:

\[
\frac{\partial C_0}{\partial w_{jk}^{[L]}} = 2 \left( a_j^{[L]} - y_j \right) \cdot \sigma' \left( z_j^{[L]} \right) \cdot a_k^{[L-1]}
\]

\paragraph{Gradienten der Kostenfunktion}

Der Gradient der Kostenfunktion \( C_0 \) in Bezug auf alle Gewichtungen wird durch folgende Matrix dargestellt:

\[
\nabla W^{[L]} C_0 = \left( 2 \left( a^{[L]} - y \right) \odot \sigma' \left( Z^{[L]} \right) \right) A^{[L-1]T}
\]

\subsubsection{Mathematische Grundlagen}

Die Gradientenmatrix für die Gewichtungen in Bezug auf die Kostenfunktion \( C_0 \) kann in Matrixform dargestellt werden:

\[
\nabla_{W^{[L]}} C_0 = 
\begin{pmatrix}
2(a_1^{[L]} - y_1) \cdot \sigma' (z_1^{[L]}) \cdot a_1^{[L-1]} & \cdots & 2(a_1^{[L]} - y_1) \cdot \sigma' (z_1^{[L]}) \cdot a_m^{[L-1]} \\
2(a_2^{[L]} - y_2) \cdot \sigma' (z_2^{[L]}) \cdot a_1^{[L-1]} & \cdots & 2(a_2^{[L]} - y_2) \cdot \sigma' (z_2^{[L]}) \cdot a_m^{[L-1]} \\
\vdots & \ddots & \vdots \\
2(a_n^{[L]} - y_n) \cdot \sigma' (z_n^{[L]}) \cdot a_1^{[L-1]} & \cdots & 2(a_n^{[L]} - y_n) \cdot \sigma' (z_n^{[L]}) \cdot a_m^{[L-1]}
\end{pmatrix}
\]

Diese Matrix gibt die Änderungsrate der Kostenfunktion \( C_0 \) in Bezug auf das entsprechende Gewicht an.



